{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#don't need to worry about any of this.\n",
      "\"\"\"\n",
      "#WE DON'T YET HAVE THE COMPLETE INFO TO DO THIS KIND OF ANALYSIS - EVERY WORD IS INCLUDED IN ALL TOPICS, BUT EACH\n",
      "# WORD HAS ITS OWN SPECIFIC PROBABILITY, WHICH HAS NOT BEEN TAKEN INTO ACCOUNT.\n",
      "\n",
      "##create document clusters, based on LDA-calculated topics\n",
      "\n",
      "from collections import defaultdict\n",
      "\n",
      "#first, create a dictionary matching each word to its list of topics.\n",
      "fclusters = open(\"LDA_topic_clusters\", \"r\")\n",
      "topic_wds = defaultdict(list)\n",
      "topic = -1\n",
      "for line in fclusters:\n",
      "    if line[0] == \"*\":\n",
      "        topic += 1\n",
      "    else:\n",
      "        wds = line.split(' ')\n",
      "        for wd in wds:\n",
      "            topic_wds[wd] += [topic]\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read statistics for each author, and try to cluster using those statistics.\n",
      "## THIS CELL IS JUST DATA PREPARATION. CAN START OFF FROM HERE, BECAUSE THIS READS IN DATA STORED EARLIER.\n",
      "import os\n",
      "import numpy\n",
      "rdfiledir = os.getcwd() + \"/reviewerwdscores/tf_df_reviewers\"\n",
      "#rdfiles = os.listdir(rdfiledir)\n",
      "\n",
      "#first, get indices for the reviewers, and get their review frequencies.\n",
      "f = open(\"generated_data/revspercritic\", \"r\")\n",
      "index = 0\n",
      "rev_inds = []\n",
      "rev_docno = []\n",
      "for line in f:\n",
      "    index += 1\n",
      "    numrevs = int(line)\n",
      "    if (numrevs >= 10) and (numrevs <= 200):\n",
      "        rev_inds = rev_inds + [index]\n",
      "        rev_docno = rev_docno + [numrevs]\n",
      "\n",
      "#now, get all the words and set up proper data structures.\n",
      "#NOTE: 392 words and 200 reviewers have been approved.\n",
      "\n",
      "#this collects the on-topic words.\n",
      "f = open(\"freq_topic_words_mod\")\n",
      "i = 0\n",
      "topicwords = []\n",
      "for line in f:\n",
      "    i += 1\n",
      "    if i > 392:\n",
      "        break\n",
      "    linearr = line.split()\n",
      "    topicwords += [linearr[0]]\n",
      "    \n",
      "#this sets up the correct data structure (n-d array) for clustering.    \n",
      "words_freqs = numpy.ndarray((199, 392), dtype = int)\n",
      "#create a dictionary that maps words to their indices.\n",
      "wordtoind = {}\n",
      "for i in range(392):\n",
      "    wordtoind = dict(zip(topicwords, range(392)))\n",
      "\n",
      "#now, populate the n-dimensional array in preparation for clustering\n",
      "#initial parameter: 100*(df(wd))/ndocs [this puts all reviews on the same scale.]\n",
      "for i in range(len(rev_inds)):\n",
      "    f = open(rdfiledir + str(rev_inds[i]), \"r\")\n",
      "    #ftxt = f.read()\n",
      "    for line in f:\n",
      "        linearr = line.split(' ')\n",
      "        wd = linearr[0]\n",
      "        df = int(linearr[2])\n",
      "        words_freqs[i, wordtoind[wd]] = (float(df)/rev_docno[i])*100\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#CLUSTERING: RELIES ON INITIAL SETUP OF THE CELL ABOVE\n",
      "\n",
      "#try k-means clustering.\n",
      "## TO-DO: TRY SEVERAL (~100?) DIFFERENT CLUSTERINGS AND PICK THE BEST ONE (I.E. LEAST RSS).\n",
      "import sys\n",
      "import sklearn.cluster\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "cls = KMeans(init=\"k-means++\", n_clusters=6)\n",
      "labels = cls.fit_predict(words_freqs)\n",
      "print cls.inertia_\n",
      "for i in range(100):\n",
      "    cl_curr = KMeans(init=\"k-means++\", n_clusters=6)\n",
      "    labels = cl_curr.fit_predict(words_freqs)\n",
      "    if cls.inertia_ > cl_curr.inertia_:\n",
      "        cls = cl_curr\n",
      "    if (i%5 == 0):\n",
      "        sys.stdout.write('.')\n",
      "\n",
      "print \"\\n\" + str(cls.inertia_)\n",
      "print labels\n",
      "\n",
      "#have to find some way of evaluating information gain here!\n",
      "i = [0]*6\n",
      "for label in labels:\n",
      "    i[label] += 1\n",
      "print i\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2711324.62417\n",
        "."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        ".\n",
        "2686701.88492"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[0 1 5 3 1 0 3 1 4 0 4 1 2 3 1 2 4 0 1 4 4 1 3 5 0 1 2 1 0 5 3 0 1 5 0 4 0\n",
        " 1 1 1 3 3 0 5 0 0 1 3 1 4 0 1 1 1 0 1 3 5 0 3 1 3 0 1 0 1 0 4 4 2 1 3 3 4\n",
        " 1 0 4 0 0 0 3 3 4 3 5 4 5 0 1 1 1 0 0 2 0 3 1 0 4 0 0 0 1 1 3 4 4 4 4 3 2\n",
        " 1 3 3 1 1 0 3 1 3 0 1 1 0 4 1 5 4 3 1 0 1 2 4 3 0 0 5 1 5 4 4 3 5 0 4 3 4\n",
        " 4 3 1 0 4 1 2 0 1 3 1 3 3 3 1 4 3 3 4 5 5 1 1 4 1 0 1 0 3 4 2 3 0 0 5 5 1\n",
        " 1 1 2 1 4 4 1 2 2 1 4 4 1 0]\n",
        "[44, 56, 12, 36, 35, 16, 0, 0]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### print out the resulting word-clusters. This means finding the top 5/10 docs in the cluster and printing their\n",
      "#top words, in sorted order.\n",
      "from operator import itemgetter\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "clusters = cls.cluster_centers_\n",
      "clno = 0\n",
      "\n",
      "fig = plt.figure()\n",
      "for cluster in clusters:\n",
      "    #print cluster\n",
      "    wds = []\n",
      "    wordno = 0\n",
      "    for item in cluster:\n",
      "        if item > 10:\n",
      "            wds += [(topicwords[wordno], item)]\n",
      "            #print topicwords[wordno] + \": \" + str(item)\n",
      "        wordno += 1\n",
      "    \n",
      "    wds.sort(key = itemgetter(1), reverse = True)\n",
      "    #for wd in wds:\n",
      "    #   print wd[0] + \": \" + str(wd[1])\n",
      "    \n",
      "    ## Now, create a bar graph for the top 20 words, and repeat for all the centroids.\n",
      "    \n",
      "    cl = fig.add_subplot(2, 3, clno)\n",
      "    graphinds = range(20)\n",
      "    barvals = [int(wd[1]) for wd in wds]\n",
      "    barvals = barvals[:20]\n",
      "    barlabs = [wd[0] for wd in wds]\n",
      "    barlabs = barlabs[:20]\n",
      "    \n",
      "    #get all the points we need to create a standard deviation, and find the standard deviation.\n",
      "    clfiles = [i for i in range(len(labels)) if labels[i] == clno] #list of document indicies in given cluster\n",
      "    clvals = words_freqs[clfiles,:]\n",
      "    barsds = []\n",
      "    for val in barvals:\n",
      "        barsds += [numpy.std(clvals[:,val])] #take stdv of column.\n",
      "    \n",
      "    \n",
      "    bars = cl.bar(graphinds, barvals, yerr = barsds)\n",
      "    cl.set_xlim(-.5, 20.5)\n",
      "    cl.set_ylim(0, 100)\n",
      "    \n",
      "    cl.set_xticks(graphinds)\n",
      "    barticks = cl.set_xticklabels(barlabs)\n",
      "    plt.setp(barticks, rotation = 90)\n",
      "    clno += 1\n",
      "    #### LOOP ####\n",
      "\n",
      "\n",
      "plt.show()\n",
      "\n",
      "### FROM THIS ANALYSIS, I HAVE FOUND A COUPLE OF NOTEWORTHY THINGS. FIRSTLY, THERE IS A LARGE AMOUNT OF INFORMATION\n",
      "### GOING INTO HOW MUCH THE AUTHORS ANY OF THE WORDS. THIS COULD CONCEIVABLY BE A USEFUL STATISTIC, BUT IT'S NOT\n",
      "### WHAT I'M REALLY LOOKING FOR.\n",
      "# FOR THIS REASON, I WANT TO TRY CLUSTERING BY RELATIVE DOCUMENT FREQUENCY OR RELATIVE TERM FREQUENCY - THAT\n",
      "# WILL, I THINK, HELP ME TEASE OUT THE INFO I'M REALLY LOOKING FOR.\n",
      "\n",
      "### SECONDLY, SOME OF THE CLUSTERS ARE HARD TO TELL APART. FOR INSTANCE, THE SAME FIVE WORDS WILL SHOW UP FOR\n",
      "### ALL CLUSTERS, IN THE SAME ORDER AND WITH SIMILAR LEVELS. THERE ARE MINUTE DIFFERENCES THAT I FOUND, BUT THESE\n",
      "### WOULD BE TEASED OUT MUCH MORE EASILY IF I COULD GROUP SIMILAR WORDS, LIKE \"ACTING\", \"ACTORS\", ETC.\n",
      "# COULD TRY SVD, OR SOME OTHER ANALYSIS OF COVARIANCE MATRICES FOR THIS.\n",
      "\n",
      "### THIRDLY, THERE IS ONE CLUSTER OF REVIEWERS IN WHICH THE WORD \"HORROR\" IS USED IN EVERY REVIEW. NOT SURE IF\n",
      "### THIS IS OK OR NOT.\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this cell is for printing only\n",
      "\n",
      "#print topic_wds['cliche']\n",
      "\"\"\"\n",
      "print rev_docno[0]\n",
      "print rev_inds[0]\n",
      "print words_freqs\n",
      "#cluster centers.\n",
      "print cls.cluster_centers_\n",
      "\"\"\"\n",
      "\n",
      "#test out the graphing\n",
      "\"\"\"\n",
      "import matplotlib.pyplot as plt\n",
      "from numpy.random import random\n",
      "fig = plt.figure()\n",
      "cl = fig.add_subplot(521)\n",
      "cl.hist(random(1000), 100)\n",
      "\n",
      "i = 10\n",
      "cl = fig.add_subplot(5, 2, i)\n",
      "cl.hist(random(1000), 100)\n",
      "plt.show()\n",
      "\"\"\"\n",
      "\n",
      "#now, experiment with printing out rows and columns and manipulating them\n",
      "import sklearn\n",
      "print sklearn.__version__\n",
      "\n",
      "print len(words_freqs[:,1]) #this method gets you the nth column or row or whatever.\n",
      "print numpy.std(words_freqs[:,1])\n",
      "\n",
      "clfiles = [i for i in range(199) if (labels[i] == 5) ] #list of document indicies in given cluster\n",
      "print clfiles\n",
      "print labels\n",
      "print numpy.shape(words_freqs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'words_freqs' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-29debcf86de9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_freqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this method gets you the nth column or row or whatever.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords_freqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'words_freqs' is not defined"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.16.0\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}